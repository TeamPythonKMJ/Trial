{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting a dataset\n",
    "\n",
    "We have 12 datasets to explore in this notebook to predict the Young's Modulus with a three-layer neuron networks model with 2000 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "import pymatgen as pymat\n",
    "import mendeleev as mendel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/\n",
    "!mkdir ./logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mp_id</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Composition</th>\n",
       "      <th>IPF</th>\n",
       "      <th>Density</th>\n",
       "      <th>Elastic_Tensor</th>\n",
       "      <th>E_above_Hull</th>\n",
       "      <th>G_VRH</th>\n",
       "      <th>K_VRH</th>\n",
       "      <th>Elastic_Anisotropy</th>\n",
       "      <th>poisson_ratio</th>\n",
       "      <th>Y_Modulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mp-1005</td>\n",
       "      <td>FeP</td>\n",
       "      <td>{'Fe': 1.0, 'P': 1.0}</td>\n",
       "      <td>0.145502</td>\n",
       "      <td>6.371739</td>\n",
       "      <td>{'G_Reuss': 130.0, 'G_VRH': 133.0, 'G_Voigt': ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>329.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mp-1009077</td>\n",
       "      <td>FeH</td>\n",
       "      <td>{'Fe': 1.0, 'H': 1.0}</td>\n",
       "      <td>0.195137</td>\n",
       "      <td>7.098567</td>\n",
       "      <td>{'G_Reuss': 91.0, 'G_VRH': 91.0, 'G_Voigt': 91...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>232.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mp-11385</td>\n",
       "      <td>YFe5</td>\n",
       "      <td>{'Y': 1.0, 'Fe': 5.0}</td>\n",
       "      <td>0.205646</td>\n",
       "      <td>7.107153</td>\n",
       "      <td>{'G_Reuss': 67.0, 'G_VRH': 68.0, 'G_Voigt': 68...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>167.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mp-1418</td>\n",
       "      <td>FeNi3</td>\n",
       "      <td>{'Fe': 1.0, 'Ni': 3.0}</td>\n",
       "      <td>0.173671</td>\n",
       "      <td>8.700536</td>\n",
       "      <td>{'G_Reuss': 90.0, 'G_VRH': 96.0, 'G_Voigt': 10...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.30</td>\n",
       "      <td>249.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mp-1570</td>\n",
       "      <td>YFe2</td>\n",
       "      <td>{'Y': 1.0, 'Fe': 2.0}</td>\n",
       "      <td>0.204319</td>\n",
       "      <td>6.872990</td>\n",
       "      <td>{'G_Reuss': 35.0, 'G_VRH': 36.0, 'G_Voigt': 37...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>95.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>206</td>\n",
       "      <td>mp-6624</td>\n",
       "      <td>Tb2Fe2Si2C</td>\n",
       "      <td>{'Tb': 2.0, 'Fe': 2.0, 'Si': 2.0, 'C': 1.0}</td>\n",
       "      <td>0.133449</td>\n",
       "      <td>7.583527</td>\n",
       "      <td>{'G_Reuss': 76.0, 'G_VRH': 81.0, 'G_Voigt': 85...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.24</td>\n",
       "      <td>200.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>207</td>\n",
       "      <td>mp-998980</td>\n",
       "      <td>TiAlFeCo</td>\n",
       "      <td>{'Ti': 1.0, 'Al': 1.0, 'Fe': 1.0, 'Co': 1.0}</td>\n",
       "      <td>0.170756</td>\n",
       "      <td>6.423142</td>\n",
       "      <td>{'G_Reuss': 127.0, 'G_VRH': 127.0, 'G_Voigt': ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>307.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>208</td>\n",
       "      <td>mp-1025316</td>\n",
       "      <td>Ho2Fe2Si2C</td>\n",
       "      <td>{'Ho': 2.0, 'Fe': 2.0, 'Si': 2.0, 'C': 1.0}</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>7.927919</td>\n",
       "      <td>{'G_Reuss': 80.0, 'G_VRH': 84.0, 'G_Voigt': 87...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.23</td>\n",
       "      <td>206.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>209</td>\n",
       "      <td>mp-1025357</td>\n",
       "      <td>Lu2Fe2Si2C</td>\n",
       "      <td>{'Lu': 2.0, 'Fe': 2.0, 'Si': 2.0, 'C': 1.0}</td>\n",
       "      <td>0.145499</td>\n",
       "      <td>8.519184</td>\n",
       "      <td>{'G_Reuss': 82.0, 'G_VRH': 85.0, 'G_Voigt': 88...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.25</td>\n",
       "      <td>212.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>210</td>\n",
       "      <td>mp-18878</td>\n",
       "      <td>LiFeAs2O7</td>\n",
       "      <td>{'Li': 1.0, 'Fe': 1.0, 'As': 2.0, 'O': 7.0}</td>\n",
       "      <td>0.498498</td>\n",
       "      <td>4.028045</td>\n",
       "      <td>{'G_Reuss': 47.0, 'G_VRH': 48.0, 'G_Voigt': 49...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>123.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       mp_id     Formula  \\\n",
       "0             0     mp-1005         FeP   \n",
       "1             1  mp-1009077         FeH   \n",
       "2             2    mp-11385        YFe5   \n",
       "3             3     mp-1418       FeNi3   \n",
       "4             4     mp-1570        YFe2   \n",
       "..          ...         ...         ...   \n",
       "206         206     mp-6624  Tb2Fe2Si2C   \n",
       "207         207   mp-998980    TiAlFeCo   \n",
       "208         208  mp-1025316  Ho2Fe2Si2C   \n",
       "209         209  mp-1025357  Lu2Fe2Si2C   \n",
       "210         210    mp-18878   LiFeAs2O7   \n",
       "\n",
       "                                      Composition       IPF   Density  \\\n",
       "0                           {'Fe': 1.0, 'P': 1.0}  0.145502  6.371739   \n",
       "1                           {'Fe': 1.0, 'H': 1.0}  0.195137  7.098567   \n",
       "2                           {'Y': 1.0, 'Fe': 5.0}  0.205646  7.107153   \n",
       "3                          {'Fe': 1.0, 'Ni': 3.0}  0.173671  8.700536   \n",
       "4                           {'Y': 1.0, 'Fe': 2.0}  0.204319  6.872990   \n",
       "..                                            ...       ...       ...   \n",
       "206   {'Tb': 2.0, 'Fe': 2.0, 'Si': 2.0, 'C': 1.0}  0.133449  7.583527   \n",
       "207  {'Ti': 1.0, 'Al': 1.0, 'Fe': 1.0, 'Co': 1.0}  0.170756  6.423142   \n",
       "208   {'Ho': 2.0, 'Fe': 2.0, 'Si': 2.0, 'C': 1.0}  0.150549  7.927919   \n",
       "209   {'Lu': 2.0, 'Fe': 2.0, 'Si': 2.0, 'C': 1.0}  0.145499  8.519184   \n",
       "210   {'Li': 1.0, 'Fe': 1.0, 'As': 2.0, 'O': 7.0}  0.498498  4.028045   \n",
       "\n",
       "                                        Elastic_Tensor  E_above_Hull  G_VRH  \\\n",
       "0    {'G_Reuss': 130.0, 'G_VRH': 133.0, 'G_Voigt': ...           0.0  133.0   \n",
       "1    {'G_Reuss': 91.0, 'G_VRH': 91.0, 'G_Voigt': 91...           0.0   91.0   \n",
       "2    {'G_Reuss': 67.0, 'G_VRH': 68.0, 'G_Voigt': 68...           0.0   68.0   \n",
       "3    {'G_Reuss': 90.0, 'G_VRH': 96.0, 'G_Voigt': 10...           0.0   96.0   \n",
       "4    {'G_Reuss': 35.0, 'G_VRH': 36.0, 'G_Voigt': 37...           0.0   36.0   \n",
       "..                                                 ...           ...    ...   \n",
       "206  {'G_Reuss': 76.0, 'G_VRH': 81.0, 'G_Voigt': 85...           0.0   81.0   \n",
       "207  {'G_Reuss': 127.0, 'G_VRH': 127.0, 'G_Voigt': ...           0.0  127.0   \n",
       "208  {'G_Reuss': 80.0, 'G_VRH': 84.0, 'G_Voigt': 87...           0.0   84.0   \n",
       "209  {'G_Reuss': 82.0, 'G_VRH': 85.0, 'G_Voigt': 88...           0.0   85.0   \n",
       "210  {'G_Reuss': 47.0, 'G_VRH': 48.0, 'G_Voigt': 49...           0.0   48.0   \n",
       "\n",
       "     K_VRH  Elastic_Anisotropy  poisson_ratio  Y_Modulus  \n",
       "0    212.0                0.28           0.24     329.84  \n",
       "1    177.0                0.03           0.28     232.96  \n",
       "2    105.0                0.13           0.23     167.28  \n",
       "3    203.0                0.73           0.30     249.60  \n",
       "4     92.0                0.25           0.33      95.76  \n",
       "..     ...                 ...            ...        ...  \n",
       "206  126.0                0.61           0.24     200.88  \n",
       "207  175.0                0.01           0.21     307.34  \n",
       "208  129.0                0.51           0.23     206.64  \n",
       "209  141.0                0.37           0.25     212.50  \n",
       "210   96.0                0.34           0.29     123.84  \n",
       "\n",
       "[211 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(Path('../All Data Export/Ti_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/C_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Cr_compounds_dataframe.csv'))\n",
    "df = pd.read_csv(Path('../All Data Export/Fe_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Hf_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Mo_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Nb_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Ni_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/V_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/W_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Zr_compounds_dataframe.csv'))\n",
    "# df = pd.read_csv(Path('../All Data Export/Ta_compounds_dataframe.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'mp_id', 'Formula', 'Composition', 'IPF', 'Density',\n",
       "       'Elastic_Tensor', 'E_above_Hull', 'G_VRH', 'K_VRH',\n",
       "       'Elastic_Anisotropy', 'poisson_ratio', 'Y_Modulus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPF</th>\n",
       "      <th>Density</th>\n",
       "      <th>E_above_Hull</th>\n",
       "      <th>G_VRH</th>\n",
       "      <th>K_VRH</th>\n",
       "      <th>Elastic_Anisotropy</th>\n",
       "      <th>poisson_ratio</th>\n",
       "      <th>Y_Modulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.145502</td>\n",
       "      <td>6.371739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>329.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.195137</td>\n",
       "      <td>7.098567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>232.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205646</td>\n",
       "      <td>7.107153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>167.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.173671</td>\n",
       "      <td>8.700536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.30</td>\n",
       "      <td>249.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204319</td>\n",
       "      <td>6.872990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>95.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.133449</td>\n",
       "      <td>7.583527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.24</td>\n",
       "      <td>200.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.170756</td>\n",
       "      <td>6.423142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>307.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.150549</td>\n",
       "      <td>7.927919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.23</td>\n",
       "      <td>206.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.145499</td>\n",
       "      <td>8.519184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.25</td>\n",
       "      <td>212.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.498498</td>\n",
       "      <td>4.028045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>123.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IPF   Density  E_above_Hull  G_VRH  K_VRH  Elastic_Anisotropy  \\\n",
       "0    0.145502  6.371739           0.0  133.0  212.0                0.28   \n",
       "1    0.195137  7.098567           0.0   91.0  177.0                0.03   \n",
       "2    0.205646  7.107153           0.0   68.0  105.0                0.13   \n",
       "3    0.173671  8.700536           0.0   96.0  203.0                0.73   \n",
       "4    0.204319  6.872990           0.0   36.0   92.0                0.25   \n",
       "..        ...       ...           ...    ...    ...                 ...   \n",
       "206  0.133449  7.583527           0.0   81.0  126.0                0.61   \n",
       "207  0.170756  6.423142           0.0  127.0  175.0                0.01   \n",
       "208  0.150549  7.927919           0.0   84.0  129.0                0.51   \n",
       "209  0.145499  8.519184           0.0   85.0  141.0                0.37   \n",
       "210  0.498498  4.028045           0.0   48.0   96.0                0.34   \n",
       "\n",
       "     poisson_ratio  Y_Modulus  \n",
       "0             0.24     329.84  \n",
       "1             0.28     232.96  \n",
       "2             0.23     167.28  \n",
       "3             0.30     249.60  \n",
       "4             0.33      95.76  \n",
       "..             ...        ...  \n",
       "206           0.24     200.88  \n",
       "207           0.21     307.34  \n",
       "208           0.23     206.64  \n",
       "209           0.25     212.50  \n",
       "210           0.29     123.84  \n",
       "\n",
       "[208 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop( df[ df['Y_Modulus'] < 0 ].index , inplace=True)\n",
    "df_original = df.copy\n",
    "elements = df['Formula'].tolist()\n",
    "df = df.drop(df.columns[[0, 1, 2, 3, 6]], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing and Organizing Data\n",
    "\n",
    "Most machine learning models are trained on a subset of all the available data, called the \"training set\", and the models are tested on the remainder of the available data, called the \"testing set\". Model performance has often been found to be enhanced when the inputs are normalized.\n",
    "\n",
    "##### SETS\n",
    "\n",
    "1. With the dataset we just created, we have splitted the dataset for each of the 12 datasets:\n",
    "- C: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Cr: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Fe: 208 entries for our model. We will train with 182 cases and test on the remaining 26 elements to estimate Young's Modulus.\n",
    "- Hf: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Mo: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Nb: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Ni: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Ta: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Ti: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- V: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- W: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "- Zr: 331 entries for our model. We will train with 300 cases and test on the remaining 31 elements to estimate Young's Modulus.\n",
    "\n",
    "\n",
    "\n",
    "##### NORMALIZATION\n",
    "\n",
    "Each one of these input data features has different units and is represented in scales with distinct orders of magnitude. Datasets that contain inputs like this need to be normalized, so that quantities with large values do not *overwhelm* the neural network, forcing it tune its weights to account for the different scales of our input data. In this work, we will use the Standard Score Normalization, which subtracts the mean of the feature and divide by its standard deviation.\n",
    "\n",
    "<span style=\"font-size:2em;\">$ \\frac{X - µ}{σ} $ </span>\n",
    "\n",
    "While our model might converge without feature normalization, the resultant model would be difficult to train and would be dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14550248040628322, 6.371739342364887, 0.0, 133.0, 212.0, 0.28, 0.24, 329.84], [0.19513738021337607, 7.098567499542508, 0.0, 91.0, 177.0, 0.03, 0.28, 232.96], [0.2056455695249951, 7.107153233920273, 0.0, 68.0, 105.0, 0.13, 0.23, 167.28], [0.17367146631257085, 8.700535614964549, 0.0, 96.0, 203.0, 0.73, 0.3, 249.6], [0.2043185026424397, 6.8729897874333465, 0.0, 36.0, 92.0, 0.25, 0.33, 95.76], [0.10435576052265744, 4.958877478506669, 0.0, 127.0, 181.0, 0.03, 0.21, 307.34], [0.19313314469317516, 7.3193844320864025, 0.0, 42.0, 86.0, 0.28, 0.29, 108.36], [0.2174494712907349, 7.421215789217515, 0.0, 96.0, 207.0, 0.04, 0.3, 249.6], [0.2214510835631901, 8.075535767806645, 0.5593906250016545, 77.0, 184.0, 0.67, 0.32, 203.28], [0.201753559860406, 7.490316374323061, 0.0, 145.0, 227.0, 0.02, 0.24, 359.6], [0.5317615508776439, 4.949992970445885, 0.0, 64.0, 176.0, 0.18, 0.34, 171.52], [0.1962233976191226, 8.032436452051366, 0.0, 54.0, 174.0, 7.62, 0.36, 146.88], [0.3230787550130327, 8.053991107072365, 0.0, 74.0, 94.0, 0.28, 0.19, 176.12], [0.10457399221341222, 5.127712739268919, 0.0, 154.0, 180.0, 0.52, 0.17, 360.36], [0.10876292493400244, 7.424133717867275, 0.0, 110.0, 134.0, 0.53, 0.18, 259.6], [0.17457803488601534, 5.619828139849753, 0.0, 13.0, 19.0, 36.92, 0.22, 31.72], [0.20425035234937108, 8.399632972357558, 0.0, 46.0, 177.0, 27.17, 0.38, 126.96], [0.19552750482387027, 8.290362608471677, 0.0, 91.0, 189.0, 1.08, 0.29, 234.78], [0.19209208810439832, 7.388613812179111, 0.0, 93.0, 212.0, 1.55, 0.31, 243.66], [0.1907674586716471, 8.452470763420122, 0.0, 92.0, 187.0, 1.35, 0.29, 237.36], [0.16780534254844753, 4.7693529463857045, 0.0, 146.0, 158.0, 0.02, 0.15, 335.7999999999999], [0.20587900032993334, 10.434599016620464, 0.0, 50.0, 83.0, 0.1, 0.25, 125.0], [0.1635337825325206, 5.791723402212628, 0.0, 95.0, 175.0, 1.07, 0.27, 241.3], [0.07426004152344727, 4.1175342188179735, 0.7281951666655573, 112.0, 138.0, 0.3, 0.18, 264.32], [0.18425897489132126, 9.670923150072896, 0.0, 76.0, 162.0, 0.9, 0.3, 197.6], [0.1998886594219398, 6.642034495803113, 0.0, 97.0, 194.0, 0.55, 0.29, 250.26], [0.144358749947598, 4.067297734527241, 0.0, 33.0, 97.0, 1.62, 0.35, 89.10000000000002], [0.1170207291147536, 3.1330774158156265, 0.0, 49.0, 104.0, 3.06, 0.3, 127.4], [0.22184363095015305, 8.548117600189117, 0.0, 65.0, 86.0, 0.32, 0.2, 156.0], [0.19327346453639424, 12.522527213669706, 0.0, 93.0, 228.0, 0.06, 0.32, 245.52], [0.09789489323990884, 3.4281382687103252, 0.0, 56.0, 106.0, 0.46, 0.28, 143.36], [0.07373298156910843, 4.0883100579794185, 0.0, 114.0, 141.0, 0.44, 0.18, 269.04], [0.20954903839243388, 8.136129054031493, 0.0, 96.0, 187.0, 0.49, 0.28, 245.76], [0.1979653804218472, 8.206826865271259, 0.0, 97.0, 190.0, 0.65, 0.28, 248.32], [0.20439360401920215, 8.108531533908858, 0.0, 98.0, 192.0, 0.59, 0.28, 250.88], [0.12552372246884655, 6.809894203032001, 0.0, 66.0, 102.0, 0.34, 0.23, 162.36], [0.1476792971076537, 16.884043971045738, 0.0, 101.0, 251.0, 0.02, 0.32, 266.64], [0.19241480430505895, 6.127073159507482, 0.0, 60.0, 267.0, 2.72, 0.39, 166.8], [0.2259465293347025, 7.0898572130344615, 0.0, 92.0, 118.0, 0.28, 0.19, 218.96], [0.1760389680764239, 7.083597008639862, 0.0, 89.0, 216.0, 0.23, 0.32, 234.96], [0.20385700488606556, 12.874922439317725, 0.4775339583318328, 177.0, 310.0, 0.07, 0.26, 446.04], [0.20810172516849768, 7.739814841627519, 0.0, 90.0, 193.0, 0.55, 0.3, 234.0], [0.14791894463886804, 6.333879896320733, 0.0, 139.0, 211.0, 0.06, 0.23, 341.94], [0.10119575205206092, 7.235385105312517, 0.0, 44.0, 82.0, 0.56, 0.27, 111.76], [0.15205066659283706, 4.042916437987318, 0.0, 82.0, 155.0, 3.48, 0.28, 209.92], [0.152978948933995, 7.921780119084059, 0.0, 57.0, 92.0, 0.87, 0.24, 141.36], [0.10222130167586677, 5.414620990655353, 0.0, 108.0, 150.0, 0.0, 0.21, 261.36], [0.20398386201178567, 4.89365155220211, 0.0, 125.0, 150.0, 0.3, 0.17, 292.5], [0.15727479443423822, 9.581024882451251, 0.0, 79.0, 197.0, 0.8, 0.32, 208.56], [0.10895342920113564, 5.2072546118558565, 0.0, 112.0, 151.0, 0.01, 0.2, 268.8], [0.14112125045217866, 7.004553603311077, 0.0, 119.0, 202.0, 0.13, 0.25, 297.5], [0.17674732082160266, 8.703471491062963, 0.0, 71.0, 168.0, 1.41, 0.32, 187.44], [0.16996073604521206, 14.814623937197833, 0.0, 92.0, 201.0, 0.69, 0.3, 239.2], [0.15772622596283298, 7.537767409339553, 0.0, 68.0, 115.0, 0.28, 0.25, 170.0], [0.13175370668852934, 6.80880443383841, 0.0, 36.0, 104.0, 0.17, 0.34, 96.48], [0.17199723440842046, 8.298256566490334, 0.0, 79.0, 117.0, 0.21, 0.22, 192.76], [0.1929636306478071, 6.569673706769142, 0.0, 80.0, 125.0, 0.25, 0.24, 198.4], [0.12759692236827402, 4.348807153271613, 0.0, 85.0, 126.0, 0.19, 0.22, 207.4], [0.15456986974137746, 4.140337296357571, 0.0, 19.0, 30.0, 47.19, 0.24, 47.12], [0.14361618887288816, 5.391728237009906, 0.0, 110.0, 172.0, 0.18, 0.24, 272.8], [0.1928314060287035, 8.008231829204718, 0.0, 75.0, 113.0, 0.2, 0.23, 184.5], [0.18172229698259232, 6.773327621235782, 0.0, 137.0, 218.0, 0.0, 0.24, 339.76], [0.15089079406668066, 8.501325575542893, 0.0, 88.0, 154.0, 0.22, 0.26, 221.76], [0.16552365154728016, 10.595547042879602, 0.0, 64.0, 232.0, 3.85, 0.37, 175.36], [0.13595772431198386, 16.164722811212954, 0.0, 83.0, 249.0, 0.19, 0.35, 224.1], [0.15279574249770486, 8.317102651224308, 0.0, 73.0, 155.0, 0.4, 0.3, 189.8], [0.055335026870076964, 3.3459219879134334, 0.0, 91.0, 115.0, 0.46, 0.19, 216.58], [0.1997474951801632, 7.045097021894572, 0.0, 48.0, 76.0, 1.29, 0.24, 119.04], [0.17942564343113718, 9.21336916297408, 0.0, 71.0, 148.0, 11.58, 0.29, 183.18], [0.19676951228902068, 8.27411915840166, 0.0, 85.0, 187.0, 0.07, 0.3, 221.0], [0.5771798647859696, 5.106824805695793, 0.0, 63.0, 111.0, 0.03, 0.26, 158.76], [0.5920539626991935, 4.208426588436105, 0.0, 72.0, 114.0, 0.3, 0.24, 178.56], [0.1842176596479672, 8.367357755094483, 0.0, 106.0, 181.0, 0.0, 0.26, 267.12], [0.14317211265699495, 10.755335245579223, 0.0, 125.0, 213.0, 0.13, 0.25, 312.5], [0.1313160118297123, 4.427777483208082, 0.0, 120.0, 178.0, 0.11, 0.22, 292.8], [0.1619870600352024, 6.226265158917527, 0.0, 70.0, 106.0, 0.42, 0.23, 172.2], [0.1801205340396293, 6.069993532662315, 0.0, 81.0, 115.0, 0.14, 0.22, 197.64], [0.2257445021865103, 5.702756767321699, 0.0, 56.0, 90.0, 0.55, 0.24, 138.88], [0.16556574698085802, 6.127574045141812, 0.0, 24.0, 38.0, 30.77, 0.25, 60.0], [0.18318550584879445, 7.323536906496803, 0.0, 65.0, 135.0, 0.34, 0.29, 167.70000000000005], [0.2903487904128355, 5.196240137983644, 0.0, 19.0, 33.0, 0.6, 0.26, 47.88], [0.1501576695810445, 5.6248780041688065, 0.0, 109.0, 183.0, 0.4, 0.25, 272.5], [0.13730043256093735, 8.448733889055742, 0.0, 81.0, 166.0, 0.26, 0.29, 208.98], [0.1017619091914366, 5.171877804843587, 0.0, 132.0, 161.0, 0.02, 0.18, 311.52], [0.16984691256283047, 7.982389200859106, 0.7931206666675905, 62.0, 129.0, 0.98, 0.29, 159.96], [0.15618313575265286, 5.211400407134747, 0.0, 40.0, 68.0, 0.49, 0.25, 100.0], [0.15666462674286738, 6.952996117476952, 0.0, 97.0, 112.0, 0.31, 0.16, 225.04], [0.291217460302899, 5.880310923396108, 0.0, 26.0, 44.0, 0.18, 0.25, 65.0], [0.18302227726688025, 6.253070135591179, 0.0, 105.0, 185.0, 0.07, 0.26, 264.6], [0.15354113742872438, 5.79199396598175, 0.0, 129.0, 203.0, 0.2, 0.24, 319.92], [0.1389395877509802, 7.200878266867101, 0.0, 82.0, 137.0, 0.2, 0.25, 205.0], [0.1754508400779713, 4.868177390439816, 0.0, 79.0, 89.0, 1.59, 0.16, 183.28], [0.15310685391173326, 6.221555948909835, 0.0, 117.0, 143.0, 0.28, 0.18, 276.12], [0.1741622516269664, 8.524073952353506, 0.0, 60.0, 143.0, 1.19, 0.32, 158.4], [0.18300581412782066, 6.7581276538375015, 0.0, 146.0, 229.0, 0.01, 0.24, 362.08], [0.2617425348093682, 5.6360249502044235, 0.0, 26.0, 49.0, 1.0, 0.27, 66.04], [0.16546743070198322, 9.628102428037316, 0.0, 91.0, 166.0, 0.01, 0.27, 231.14], [0.2166933250015826, 3.222255763827941, 0.0, 47.0, 76.0, 0.05, 0.25, 117.5], [0.1249780184086204, 4.7611336384527565, 0.0, 45.0, 72.0, 0.19, 0.24, 111.6], [0.15680959796955354, 7.821976730972777, 0.0, 74.0, 123.0, 0.19, 0.25, 185.0], [0.6339850627730784, 6.104829379579924, 0.0, 75.0, 116.0, 0.18, 0.24, 186.0], [0.1579303912105199, 3.8747953902789183, 0.0, 51.0, 80.0, 0.76, 0.24, 126.48], [0.17001763779218046, 9.65994186354958, 0.0, 72.0, 126.0, 0.24, 0.26, 181.44], [0.1557575610897289, 6.792241310030813, 0.0, 32.0, 88.0, 6.39, 0.34, 85.76], [0.1458053346377448, 9.280268498520932, 0.0, 82.0, 168.0, 1.94, 0.29, 211.56], [0.12595933635788814, 5.1432356119067775, 0.0, 116.0, 180.0, 0.03, 0.24, 287.68], [0.15446623864784298, 11.1212707640574, 0.0, 91.0, 197.0, 0.25, 0.3, 236.6], [0.1199095288130903, 2.7767773872587216, 0.0, 61.0, 79.0, 0.01, 0.19, 145.18], [0.15386968756067726, 6.676438879050527, 0.0, 76.0, 144.0, 0.24, 0.28, 194.56], [0.11957588545877705, 4.124423887485785, 0.0, 75.0, 120.0, 0.11, 0.24, 186.0], [0.18785084506156496, 6.319998733248389, 0.0, 42.0, 78.0, 1.1, 0.27, 106.68], [0.17560167243740976, 7.112631276806603, 0.0, 44.0, 246.0, 22.73, 0.42, 124.96], [0.4620488694594185, 5.361834298244193, 0.0, 50.0, 143.0, 2.4, 0.34, 134.0], [0.16975722021180134, 7.290370076772892, 0.0, 127.0, 216.0, 0.09, 0.25, 317.5], [0.13636325971507504, 15.144160510755146, 0.0, 94.0, 196.0, 0.15, 0.29, 242.52], [0.21125710640798706, 7.348434423639102, 0.0, 107.0, 232.0, 0.68, 0.3, 278.2], [0.2797864537394527, 7.544316549414457, 0.0, 80.0, 153.0, 0.04, 0.28, 204.8], [0.14052708255416846, 6.364388156200223, 0.0, 42.0, 61.0, 2.04, 0.22, 102.48], [0.14630713364471787, 5.6743866817132185, 0.0, 35.0, 59.0, 0.25, 0.25, 87.5], [0.5342803459794407, 7.090244770279603, 0.0, 54.0, 141.0, 0.62, 0.33, 143.64], [0.14664585366594407, 11.640108246456196, 0.0, 143.0, 264.0, 0.1, 0.27, 363.22], [0.19410324245881008, 4.793908386613678, 0.0, 32.0, 50.0, 0.9, 0.23, 78.72], [0.13951689756665347, 4.848691540330628, 0.0, 62.0, 139.0, 0.62, 0.31, 162.44], [0.17341104708130356, 6.473381427944479, 0.0, 98.0, 230.0, 0.97, 0.31, 256.76], [0.1663795746323075, 7.019572565285538, 0.0, 122.0, 196.0, 0.67, 0.24, 302.56], [0.1801640555763463, 8.715163774193382, 0.0, 35.0, 60.0, 0.24, 0.25, 87.5], [0.6235820148808287, 5.3056988684625415, 0.0, 83.0, 131.0, 0.0, 0.24, 205.84], [0.17038328912943895, 8.830491042709065, 0.0, 51.0, 126.0, 0.76, 0.32, 134.64], [0.15988294526845548, 7.393942769723793, 0.0, 97.0, 233.0, 2.61, 0.32, 256.08], [0.20882032009483695, 6.446281180426621, 0.0, 42.0, 77.0, 0.59, 0.27, 106.68], [0.1969910165773528, 7.798866002382084, 0.0, 100.0, 190.0, 0.32, 0.28, 256.0], [0.1347054325552005, 8.787100304790068, 0.0, 38.0, 193.0, 29.81, 0.41, 107.16], [0.17020698849289725, 6.549536522803793, 0.0, 78.0, 180.0, 0.63, 0.31, 204.36], [0.1679079732267066, 7.347107032217942, 0.0, 124.0, 212.0, 0.0, 0.26, 312.48], [0.16939683382649162, 11.322827521885165, 0.0, 107.0, 179.0, 0.0, 0.25, 267.5], [0.190944595199784, 7.415098713774526, 0.0, 60.0, 114.0, 0.09, 0.28, 153.6], [0.2337012095749796, 3.770900712859419, 0.0, 8.0, 18.0, 2.2, 0.31, 20.96], [0.19358538383855708, 11.036221026167702, 0.0, 132.0, 233.0, 0.55, 0.26, 332.64], [0.24424175742746204, 2.4799371669257533, 0.0, 69.0, 84.0, 0.02, 0.17, 161.45999999999995], [0.1941346317857757, 4.9089042046840845, 0.0, 32.0, 68.0, 1.23, 0.29, 82.56], [0.4968106341675815, 4.77457823904019, 0.0, 59.0, 158.0, 0.63, 0.33, 156.94], [0.5337605861407245, 4.943361645847292, 0.0, 55.0, 109.0, 0.73, 0.28, 140.8], [0.16988194597056885, 7.1079597310891485, 0.0, 57.0, 126.0, 0.38, 0.3, 148.20000000000005], [0.13357415184886798, 4.612205621434769, 0.0, 85.0, 123.0, 0.17, 0.22, 207.4], [0.18136902482679093, 7.54758423829157, 0.0, 112.0, 180.0, 0.13, 0.24, 277.76], [0.14346294918194893, 7.8237252908534005, 0.0, 62.0, 160.0, 0.49, 0.33, 164.92000000000004], [0.6281668601388051, 3.628809075015994, 0.0, 63.0, 99.0, 0.79, 0.24, 156.24], [0.15084992827688273, 5.977171478082613, 0.0, 117.0, 181.0, 0.24, 0.23, 287.82], [0.1236201319787054, 12.301920219995141, 0.0, 105.0, 236.0, 0.08, 0.31, 275.1], [0.1920003220884418, 6.823927950272397, 0.0, 110.0, 222.0, 0.16, 0.29, 283.8], [0.1831303139670774, 11.799447669082904, 0.0, 124.0, 224.0, 0.03, 0.27, 314.96], [0.1463283420235767, 13.620045837210329, 0.0, 98.0, 218.0, 0.6, 0.3, 254.8], [0.14279689597396342, 7.055233350704827, 0.0, 53.0, 114.0, 0.11, 0.3, 137.8], [0.16947493920527334, 5.9674379365622645, 0.0, 75.0, 120.0, 0.25, 0.24, 186.0], [0.12096057843147195, 7.276886289223003, 0.0, 58.0, 113.0, 0.11, 0.28, 148.48], [0.1343122029832427, 7.506788296223648, 0.0, 54.0, 88.0, 0.88, 0.25, 135.0], [0.1926313797461748, 8.260081041652537, 0.0, 122.0, 225.0, 0.03, 0.27, 309.88], [0.19550052751793975, 5.2294993112533685, 0.0, 57.0, 92.0, 1.12, 0.24, 141.36], [0.16826154390583853, 7.34345070105442, 0.0, 84.0, 149.0, 0.17, 0.26, 211.68], [0.2543008406930701, 3.1586920367926523, 0.0, 7.0, 17.0, 5.57, 0.31, 18.34], [0.1915959768968402, 10.875201077103958, 0.0, 100.0, 214.0, 0.08, 0.3, 260.0], [0.4391245871633099, 3.385648190934757, 0.4583850000017264, 27.0, 44.0, 1.68, 0.25, 67.5], [0.13580521487181216, 9.665034987039776, 0.0, 99.0, 182.0, 0.66, 0.27, 251.46], [0.19972684237916305, 7.947002590025137, 0.0, 127.0, 215.0, 0.0, 0.25, 317.5], [0.17818629265494992, 8.5873031821058, 0.0, 51.0, 217.0, 40.98, 0.39, 141.78], [0.13972301430837555, 3.7243295083372905, 0.0, 45.0, 79.0, 0.7, 0.26, 113.4], [0.6047509602150205, 4.108906898895016, 0.0, 93.0, 174.0, 0.54, 0.27, 236.22], [0.4799470645391793, 6.307568894088197, 0.0, 41.0, 131.0, 4.98, 0.36, 111.52], [0.17598792325122878, 7.731271210689283, 0.0, 55.0, 92.0, 0.89, 0.25, 137.5], [0.1183066392181886, 7.8101188244117585, 0.0, 56.0, 88.0, 0.32, 0.24, 138.88], [0.17704786353696056, 8.691986889856317, 0.0, 81.0, 191.0, 1.87, 0.31, 212.22], [0.10981100280290496, 9.193008838162203, 0.0, 116.0, 237.0, 0.02, 0.29, 299.2800000000001], [0.14956770032236896, 5.6830162683780205, 0.0, 80.0, 140.0, 0.13, 0.26, 201.6], [0.1735182706004257, 8.189445893753442, 0.0, 77.0, 100.0, 0.22, 0.19, 183.26], [0.5253411617198392, 5.684689240639937, 0.0, 74.0, 173.0, 0.42, 0.31, 193.88], [0.1702747319563653, 6.269390828054855, 0.0, 122.0, 223.0, 0.43, 0.27, 309.88], [0.14258118802711173, 14.898057437222654, 0.0, 65.0, 188.0, 0.0, 0.34, 174.20000000000005], [0.1535804375798964, 5.968421754413581, 0.0, 53.0, 101.0, 0.55, 0.28, 135.68], [0.16561678757850282, 7.183512710543788, 0.0, 98.0, 188.0, 1.01, 0.28, 250.88], [0.11593527357972633, 5.979508612096697, 0.6823062777749911, 48.0, 105.0, 0.0, 0.3, 124.8], [0.5363093054539325, 3.471794177389916, 0.6200458333314174, 39.0, 73.0, 0.32, 0.27, 99.06], [0.20752190866567607, 6.57013858893239, 0.0, 45.0, 59.0, 1.58, 0.2, 108.0], [0.2031953563609596, 6.917186606244874, 0.0, 143.0, 238.0, 0.05, 0.25, 357.5], [0.16008144939291774, 3.626600190170163, 0.0, 33.0, 72.0, 0.77, 0.3, 85.8], [0.18485878347620224, 8.264551981653039, 0.0, 83.0, 200.0, 0.16, 0.32, 219.12], [0.16027200221524826, 7.040081781783568, 0.0, 86.0, 113.0, 2.15, 0.2, 206.4], [0.16811635255747065, 9.950049065017925, 0.0, 72.0, 191.0, 0.04, 0.33, 191.52], [0.16952002333910032, 10.2376920474426, 0.0, 137.0, 222.0, 0.0, 0.24, 339.76], [0.16212246129176208, 6.289050035764848, 0.0, 124.0, 216.0, 0.1, 0.26, 312.48], [0.1744796793969142, 7.804425157940714, 0.0, 83.0, 127.0, 0.53, 0.23, 204.18], [0.5281605073077549, 3.4708958823634912, 0.0, 48.0, 91.0, 0.17, 0.28, 122.88], [0.3131620163600146, 2.9758159256131105, 0.0, 30.0, 99.0, 0.0, 0.36, 81.6], [0.17379314326416934, 3.310495649052089, 0.0, 39.0, 54.0, 0.18, 0.21, 94.38], [0.4993786355659096, 3.129272616958105, 0.0, 44.0, 88.0, 0.28, 0.28, 112.64], [0.3500421155008848, 3.4268223606196653, 0.0, 23.0, 49.0, 0.34, 0.3, 59.8], [0.14906151429182124, 8.070122738219048, 0.0, 85.0, 139.0, 0.45, 0.25, 212.5], [0.5374331018738084, 3.5783228299071186, 0.0, 42.0, 84.0, 0.46, 0.29, 108.36], [0.1815656586239428, 7.852732359070375, 0.0, 116.0, 204.0, 0.04, 0.26, 292.32], [0.19485406368576572, 7.012711217690293, 0.0, 72.0, 128.0, 0.91, 0.26, 181.44], [0.14480703268438752, 2.4855974917700445, 0.0, 44.0, 61.0, 0.01, 0.21, 106.48], [0.1685114178255008, 8.221256348591078, 0.0, 86.0, 136.0, 0.46, 0.24, 213.28], [0.14909469323306018, 5.518674272107517, 0.0, 83.0, 126.0, 0.5, 0.23, 204.18], [0.3519389828897249, 3.778508501013152, 0.0, 31.0, 57.0, 0.0, 0.27, 78.74], [0.13344869829192668, 7.583527316438486, 0.0, 81.0, 126.0, 0.61, 0.24, 200.88], [0.1707558408601182, 6.4231420270626645, 0.0, 127.0, 175.0, 0.01, 0.21, 307.34], [0.15054940510822462, 7.927919170981369, 0.0, 84.0, 129.0, 0.51, 0.23, 206.64], [0.14549856856572418, 8.519183743668963, 0.0, 85.0, 141.0, 0.37, 0.25, 212.5], [0.4984981386663735, 4.028044539275526, 0.0, 48.0, 96.0, 0.34, 0.29, 123.84]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPF</th>\n",
       "      <th>Density</th>\n",
       "      <th>E_above_Hull</th>\n",
       "      <th>G_VRH</th>\n",
       "      <th>K_VRH</th>\n",
       "      <th>Elastic_Anisotropy</th>\n",
       "      <th>poisson_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.145502</td>\n",
       "      <td>6.371739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.195137</td>\n",
       "      <td>7.098567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205646</td>\n",
       "      <td>7.107153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.173671</td>\n",
       "      <td>8.700536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204319</td>\n",
       "      <td>6.872990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.104356</td>\n",
       "      <td>4.958877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>127.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.193133</td>\n",
       "      <td>7.319384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.217449</td>\n",
       "      <td>7.421216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.221451</td>\n",
       "      <td>8.075536</td>\n",
       "      <td>0.559391</td>\n",
       "      <td>77.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.201754</td>\n",
       "      <td>7.490316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IPF   Density  E_above_Hull  G_VRH  K_VRH  Elastic_Anisotropy  \\\n",
       "0  0.145502  6.371739      0.000000  133.0  212.0                0.28   \n",
       "1  0.195137  7.098567      0.000000   91.0  177.0                0.03   \n",
       "2  0.205646  7.107153      0.000000   68.0  105.0                0.13   \n",
       "3  0.173671  8.700536      0.000000   96.0  203.0                0.73   \n",
       "4  0.204319  6.872990      0.000000   36.0   92.0                0.25   \n",
       "5  0.104356  4.958877      0.000000  127.0  181.0                0.03   \n",
       "6  0.193133  7.319384      0.000000   42.0   86.0                0.28   \n",
       "7  0.217449  7.421216      0.000000   96.0  207.0                0.04   \n",
       "8  0.221451  8.075536      0.559391   77.0  184.0                0.67   \n",
       "9  0.201754  7.490316      0.000000  145.0  227.0                0.02   \n",
       "\n",
       "   poisson_ratio  \n",
       "0           0.24  \n",
       "1           0.28  \n",
       "2           0.23  \n",
       "3           0.30  \n",
       "4           0.33  \n",
       "5           0.21  \n",
       "6           0.29  \n",
       "7           0.30  \n",
       "8           0.32  \n",
       "9           0.24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_values = df.values.tolist() # Values for Attributes\n",
    "print(all_values)\n",
    "all_labels = [] # Values for Young's Modulus (Property to be estimated)\n",
    "all_labels = df['Y_Modulus'].tolist()\n",
    "df = df.drop(['Y_Modulus'], axis=1)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Values: (208, 7)\n",
      "Shape of Labels: (208,)\n",
      "[-0.48669484 -0.29756573 -0.19637078  1.66732805  1.10222919 -0.25796116\n",
      " -0.57256081]\n",
      "[ 0.02848216 -0.08052991 -0.19637078  1.97650146  1.54744886 -0.29254878\n",
      " -0.36903334]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207]\n"
     ]
    }
   ],
   "source": [
    "#We will rewrite the arrays with the patches we made on the dataset \n",
    "# by turning the dataframe back into a list of lists\n",
    "\n",
    "all_values = [list(df.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "# SETS\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = float)\n",
    "print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Uncomment the line below to shuffle the dataset \n",
    "#(we do not do this here to ensure consistent results for every run)\n",
    "#order = np.argsort(np.random.random(all_labels.shape)) \n",
    "# This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "order = np.arange(208)\n",
    "all_values = all_values[order]\n",
    "all_labels = all_labels[order]\n",
    "\n",
    "# Training Set\n",
    "train_labels = all_labels[:182]\n",
    "train_values = all_values[:182]\n",
    "\n",
    "# Testing Set\n",
    "test_labels = all_labels[-26:]\n",
    "test_values = all_values[-26:]\n",
    "\n",
    "# This line is used for labels in the plots at the end of the tutorial - Testing Set\n",
    "\n",
    "labeled_elements = [elements[x] for x in order[-26:]] \n",
    "elements = [elements[x] for x in order]\n",
    "\n",
    "# NORMALIZATION\n",
    "\n",
    "mean = np.mean(train_values, axis = 0) # mean\n",
    "std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "print(train_values[0]) # print a sample entry from the training set\n",
    "print(test_values[0]) # print a sample entry from the training set\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Model\n",
    "\n",
    "For this regression, we will use a simple sequential neural network with one densely connected hidden layer. The optimizer used will be [RMSPropOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) (Root Mean Square Propagation).\n",
    "\n",
    "To learn more about Root Mean Squared Propagation, click [here](https://climin.readthedocs.io/en/latest/rmsprop.html).\n",
    "\n",
    "A cool tool developed by Tensorflow to visualize how a neural network learns, and play around with its parameters, can be found here [NN Tools](https://playground.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kayla Yano\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,593\n",
      "Trainable params: 6,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "# In a sequential model, the first layer must specify the input shape the model will expect; \n",
    "# in this case the value is train_values.shape[1] which is the number\n",
    "# of attributes (properties) and equals 17.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "optimizer = optimizers.RMSprop(0.002) # Root Mean Squared Propagation\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING \n",
    "\n",
    "This model is trained for 2000 epochs, and we record the training accuracy in the history object.\n",
    "\n",
    "One **Epoch** occurs when you pass the entire dataset through the model. One **Batch** contains a subset of the dataset that can be fed to the model at the same time. A more detailed explanation of these concepts can be found in this [blog](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9). As we have a really small dataset compared to the ones that are usually considered to be modeled by these neural networks, we are feeding all entries at the same time, so our batch is the entire dataset, and an epoch occurs when the batch is processed.\n",
    "\n",
    "This way, by plotting \"history\" we can see the evolution of the \"learning\" of the model, that is the decrease of the Mean Absolute Error. Models in Keras are fitted to the training set using the [**fit**](https://keras.io/models/model/#fit) method.\n",
    "\n",
    "The blue curve that will come up from the History object represents how the model is learning on the training data, and the orange curve represents the validation loss, which can be thought of as the way our model evaluates data that it was not trained in. This validation loss would start going up again when we start to overfit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 2000 Training Loss: 4.226048                                       \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(tf.keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 2000 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "# Load call back and log entry\n",
    "log_dir = \"logs/fit/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, validation_split=0.1, callbacks=[PrintEpNum(), tensorboard_callback])\n",
    "\n",
    "\n",
    "# PLOTTING HISTORY USING MATPLOTLIB\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Abs Error')\n",
    "plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),label='Loss on training set') \n",
    "plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVING A MODEL\n",
    "\n",
    "Compiled and trained models in Keras can be saved and distributed in .h5 files using the `model.save()` method. Running the cell below will save the current model we trained, both weights and architecture to your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.expanduser('~/model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING\n",
    "\n",
    "Models in Keras are tested using the method [**evaluate**](https://keras.io/models/model/#evaluate). This method returns the testing loss of the model and the metrics we specified when creating it,  which in our case it's the Mean Absolute Error. For the original model in this tutorial you should get a value of around **20 GPa** for the Mean Absolute Error. This value would decrease with more training data, more attributes/features, or a different optimizer. In the case of a model that overfits, you can expect values to start increasing to around **30 or 40 GPa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Mean Absolute Error: 3.433 GPa\n"
     ]
    }
   ],
   "source": [
    "[loss, mae] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing Set Mean Absolute Error: {:2.3f} GPa\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v1.summary' from 'C:\\\\Users\\\\Kayla Yano\\\\anaconda3\\\\envs\\\\PythonData\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1\\\\summary\\\\__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe.ipynb\n",
      "Neuron Networks.ipynb\n",
      "Results\n",
      "logs\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 2292."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKING PREDICTIONS\n",
    "\n",
    "The last step in a regression model is to make predictions for values not in the training set, which are determined by the method [**predict**](https://keras.io/models/model/#predict). In the following cell we print the elements in the testing set, the real values for their Young's moduli and the predictions generated by our machine learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_values).flatten()\n",
    "\n",
    "print(\"Elements in Test Set: \", labeled_elements)\n",
    "print(\"Real Values\", list(test_labels))\n",
    "print(\"Predictions\", list(test_predictions))\n",
    "\n",
    "values = np.concatenate((train_values, test_values), axis=0) # This line joins the values together to evaluate all of them\n",
    "predictions = model.predict(values).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plotting\n",
    "\n",
    "The easiest way to see if the model did a good job estimating the Young's Modulus for the Elements is through a plot comparing Real Values with their Predictions. We will use [Plotly](https://plot.ly/python/) to create a plot like that. We covered how to plot in Plotly in the first tutorial of this tool. For values in this plot, the line (x = y) indicates a perfect match and would be the desirable result for the points. As you analyze the plot, you can hover on the points to see the data we obtained in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "layout0= go.Layout(title=go.layout.Title(text=\"Neural Network Model - Fe compounds Young's Modulus\", font=dict(size=28)), hovermode= 'closest', width = 1000, height=600, showlegend=True,  # Hovermode establishes the way the labels that appear when you hover are arranged # Establishing a square plot width=height\n",
    "    xaxis= dict(title=go.layout.xaxis.Title(text='Real Values (GPa)', font=dict(size=24)), zeroline= False, gridwidth= 1, tickfont=dict(size=18)), # Axis Titles. Removing the X-axis Mark. Adding a Grid\n",
    "    yaxis= dict(title=go.layout.yaxis.Title(text='Prediction (GPa)', font=dict(size=24)), zeroline= False, gridwidth= 1, tickfont=dict(size=18)), # Axis Titles. Removing the Y-axis Mark. Adding a Grid\n",
    "    legend=dict(font=dict(size=24))) # Adding a legend\n",
    "\n",
    "trace0 = go.Scatter(x = all_labels, y = predictions, mode = 'markers', marker= dict(size= 12, color= 'blue'), text= elements, name = 'Young\\'s Modulus (Training)')\n",
    "trace1 = go.Scatter(x = test_labels, y = test_predictions, mode = 'markers', marker= dict(size= 12, color= 'red'), text = labeled_elements, name = 'Young\\'s Modulus (Testing)')\n",
    "trace2 = go.Scatter(x = [0,600], y = [0,600], mode = 'lines', name = \"Match\") # This trace is the line X = Y which would indicate that the Prediction equals the real value\n",
    "\n",
    "data = [trace0, trace1, trace2]\n",
    "fig= go.Figure(data, layout=layout0)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(values, predictions,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.summary.summary.image(name, tensor, max_outputs=3, collections=None, family=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
